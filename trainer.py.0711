import os
import sys
import pathlib
import torch
import glob
import tqdm
import time
from torch.utils.data import DataLoader
from dataset.dataloader import CINE2DT
from model.k_interpolator import KInterpolator
from losses import CriterionKGIN
from utils import count_parameters, Logger, adjust_learning_rate as adjust_lr, NativeScalerWithGradNormCount as NativeScaler, add_weight_decay

import wandb

# os.environ["CUDA_VISIBLE_DEVICES"] = "3" #,0,1,2,4,5,6,7
os.environ["CUDA_VISIBLE_DEVICES"] = "1" #,0,1,2,4,5,6,7
# device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

print(f"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}")
print(f"Using device: {device}")
print(f"Actual GPU being used: torch.cuda.current_device() -> {torch.cuda.current_device()}")
print(f"GPU name: {torch.cuda.get_device_name(device)}")

class TrainerAbstract:
    def __init__(self, config):
        super().__init__()
        self.config = config.general
        self.debug = config.general.debug
        if self.debug: config.general.exp_name = 'test'
        self.experiment_dir = os.path.join(config.general.exp_save_root, config.general.exp_name)
        pathlib.Path(self.experiment_dir).mkdir(parents=True, exist_ok=True)

        self.start_epoch = 0
        self.only_infer = config.general.only_infer
        self.num_epochs = config.training.num_epochs if config.general.only_infer is False else 1

        # data
        train_ds = CINE2DT(config=config.data, mode='train')
        test_ds = CINE2DT(config=config.data, mode='val')
        self.train_loader = DataLoader(dataset=train_ds, num_workers=config.training.num_workers, drop_last=False,
                                       pin_memory=True, batch_size=config.training.batch_size, shuffle=True)
        self.test_loader = DataLoader(dataset=test_ds, num_workers=0, drop_last=False, batch_size=1, shuffle=False)

        # network
        self.network = getattr(sys.modules[__name__], config.network.which)(eval('config.network'))
        self.network.initialize_weights()
        self.network.cuda()
        print("Parameter Count: %d" % count_parameters(self.network))

        # optimizer
        param_groups = add_weight_decay(self.network, config.training.optim_weight_decay)
        self.optimizer = eval(f'torch.optim.{config.optimizer.which}')(param_groups, **eval(f'config.optimizer.{config.optimizer.which}').__dict__)

        if config.training.restore_ckpt: self.load_model(config.training)
        self.loss_scaler = NativeScaler()

    def load_model(self, args):

        if os.path.isdir(args.restore_ckpt):
            args.restore_ckpt = max(glob.glob(f'{args.restore_ckpt}/*.pth'), key=os.path.getmtime)
        ckpt = torch.load(args.restore_ckpt)
        self.network.load_state_dict(ckpt['model'], strict=True)

        print("Resume checkpoint %s" % args.restore_ckpt)
        if args.restore_training:
            self.optimizer.load_state_dict(ckpt['optimizer_state_dict'])
            self.start_epoch = ckpt['epoch'] + 1
            # self.loss_scaler.load_state_dict(ckpt['scaler'])
            print("With optim & sched!")

    def save_model(self, epoch):
        ckpt = {'epoch': epoch,
                'model': self.network.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                # 'scaler': self.loss_scaler.state_dict()
                }
        torch.save(ckpt, f'{self.experiment_dir}/model_{epoch+1:03d}.pth')


class TrainerKInterpolator(TrainerAbstract):

    def __init__(self, config):
        super().__init__(config=config)
        self.train_criterion = CriterionKGIN(config.train_loss)
        self.eval_criterion = CriterionKGIN(config.eval_loss)
        self.logger = Logger()
        self.scheduler_info = config.scheduler

    def run(self):
        pbar = tqdm.tqdm(range(self.start_epoch, self.num_epochs))
        for epoch in pbar:
            self.logger.reset_metric_item()
            start_time = time.time()
            if not self.only_infer:
                self.train_one_epoch(epoch)
            self.run_test()
            self.logger.update_metric_item('train/epoch_runtime', (time.time() - start_time)/60)
            if epoch % self.config.weights_save_frequency == 0 and not self.debug and epoch > 150:
                self.save_model(epoch)
            if epoch == self.num_epochs - 1:
                self.save_model(epoch)
            if not self.debug:
                self.logger.wandb_log(epoch)

    def train_one_epoch(self, epoch):
        self.network.train()
        for i, (kspace, coilmaps, sampling_mask) in enumerate(self.train_loader):
            # print(f"Batch {i}:")
            # print(f"kspace shape: {kspace.shape}")
            # print(f"coilmaps shape: {coilmaps.shape}")
            # print(f"sampling_mask shape: {sampling_mask.shape}")
            
            # # 打印前几个元素以查看数据内容
            # print(f"kspace sample: {kspace[0, 0, ...]}")
            # print(f"coilmaps sample: {coilmaps[0, 0, ...]}")
            # print(f"sampling_mask sample: {sampling_mask[0, 0, ...]}")
            wandb.log({"Batch:":i})
            wandb.log({"kspace shape": kspace.shape})
            wandb.log({"coilmaps shape:":coilmaps.shape})
            wandb.log({"sampling_mask shape:":sampling_mask.shape})
            
            # 打印前几个元素以查看数据内容
            wandb.log({"kspace sample:": kspace[0, 0, ...]})
            wandb.log({"coilmaps sample:": coilmaps[0, 0, ...]})
            wandb.log({"sampling_mask sample:": sampling_mask[0, 0, ...]})
            
            # 如果你只想查看前几个批次，可以设置一个条件来中断循环
            if i >= 5:  # 假设我们只想查看前5个批次
                break
            
        # 假设你想查看前3个批次
        # num_batches_to_view = 3
        # for i, batch in enumerate(self.train_loader):
        #     # 打印批次索引
        #     print(f"Batch {i}:")
            
        #     # 打印批次中的数据
        #     # 这里假设每个批次是一个字典，包含多个样本
        #     for sample_index, sample in enumerate(batch):
        #         # 打印样本索引和样本内容
        #         print(f"Sample {sample_index}:")
        #         for element_name, element in sample.items():
        #             print(f"   {element_name}: {element.shape}")
            
        #     # 如果已经查看了指定数量的批次，则退出循环
        #     if i >= num_batches_to_view - 1:
        #         break
            # for item in batch[0]:
            #     print('batch[0]:',batch[0])
            #     print(type(item), item)
            # kspace, sampling_mask = [item.cuda() for item in batch[0]][:]
            # print('kspace:',kspace)
            # print('sampling_mask:',sampling_mask)
            # ref = batch[1][0].cuda()

            # self.optimizer.zero_grad()
            # adjust_lr(self.optimizer, i/len(self.train_loader) + epoch, self.scheduler_info)

            # with torch.cuda.amp.autocast(enabled=False):
            #     k_recon_2ch, im_recon = self.network(kspace, mask=sampling_mask)  # size of kspace and mask: [B, T, H, W]
            #     sampling_mask = sampling_mask.repeat_interleave(kspace.shape[2], 2)
            #     ls = self.train_criterion(k_recon_2ch, torch.view_as_real(kspace), im_recon, ref, kspace_mask=sampling_mask)

            #     self.loss_scaler(ls['k_recon_loss_combined'], self.optimizer, parameters=self.network.parameters())

            # self.logger.update_metric_item('train/k_recon_loss', ls['k_recon_loss'].item()/len(self.train_loader))
            # self.logger.update_metric_item('train/recon_loss', ls['photometric'].item()/len(self.train_loader))

    def run_test(self):
        self.network.eval()
        with torch.no_grad():
            for i, batch in enumerate(self.test_loader):
                print("batch-0",batch[0])
                print([item.cuda() for item in batch[0]])
                kspace, sampling_mask = [item.cuda() for item in batch[0]][:]
                ref = batch[1][0].cuda()

                k_recon_2ch, im_recon = self.network(kspace, mask=sampling_mask) # size of kspace and mask: [B, T, H, W]
                k_recon_2ch = k_recon_2ch[-1]

                kspace_complex = torch.view_as_complex(k_recon_2ch)
                sampling_mask = sampling_mask.repeat_interleave(kspace.shape[2], 2)

                ls = self.eval_criterion([kspace_complex], kspace, im_recon, ref, kspace_mask=sampling_mask, mode='test')

                self.logger.update_metric_item('val/k_recon_loss', ls['k_recon_loss'].item()/len(self.test_loader))
                self.logger.update_metric_item('val/recon_loss', ls['photometric'].item()/len(self.test_loader))
                self.logger.update_metric_item('val/psnr', ls['psnr'].item()/len(self.test_loader))

            self.logger.update_best_eval_results(self.logger.get_metric_value('val/psnr'))
            self.logger.update_metric_item('train/lr', self.optimizer.param_groups[0]['lr'])